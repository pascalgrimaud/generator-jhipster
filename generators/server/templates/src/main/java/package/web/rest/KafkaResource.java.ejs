<%#
 Copyright 2013-2019 the original author or authors from the JHipster project.

 This file is part of the JHipster project, see https://www.jhipster.tech/
 for more information.

 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
-%>
package <%= packageName %>.web.rest;

import <%= packageName %>.config.KafkaProperties;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.errors.WakeupException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.http.HttpStatus;
import org.springframework.web.bind.annotation.*;
import org.springframework.web.server.ResponseStatusException;

import java.time.Duration;
import java.time.Instant;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ExecutionException;

@RestController
@RequestMapping("/api/<%= dasherizedBaseName %>-kafka")
public class <%= upperFirstCamelCase(baseName) %>KafkaResource {

    private final Logger log = LoggerFactory.getLogger(<%= upperFirstCamelCase(baseName) %>KafkaResource.class);

    private final KafkaProperties kafkaProperties;
    private KafkaProducer<String, String> producer;
    private Map<String, SafeKafkaConsumer> consumers = new ConcurrentHashMap<>();

    public <%= upperFirstCamelCase(baseName) %>KafkaResource(KafkaProperties kafkaProperties) {
        this.kafkaProperties = kafkaProperties;
        this.producer = new KafkaProducer<>(kafkaProperties.getProducerProps());
    }

    @PostMapping(value = "/publish/{topic}")
    public PublishResult publish(@PathVariable String topic, @RequestParam String message, @RequestParam(required = false) String key) throws ExecutionException, InterruptedException {
        log.debug("REST request to send to Kafka topic {} with key {} the message : {}", topic, key, message);
        RecordMetadata metadata = producer.send(new ProducerRecord<>(topic, key, message)).get();
        return new PublishResult(metadata.topic(), metadata.partition(), metadata.offset(), Instant.ofEpochMilli(metadata.timestamp()));
    }

    @PostMapping("/consumers")
    public void createConsumer(@RequestParam String name, @RequestParam("topic") List<String> topics, @RequestParam Map<String, String> params) {
        log.debug("REST request to create a Kafka consumer {} of Kafka topics {}", name, topics);
        Map<String, Object> consumerProps = kafkaProperties.getConsumerProps();
        consumerProps.putAll(params);
        consumerProps.remove("topic");
        consumerProps.remove("name");
        SafeKafkaConsumer consumer = new SafeKafkaConsumer(consumerProps);
        consumer.subscribe(topics);
        consumers.put(name, consumer);
    }

    @GetMapping("/consumers/{name}/records")
    public List<String> pollConsumer(@PathVariable String name, @RequestParam(defaultValue = "1000") int durationMs) {
        log.debug("REST request to get records of Kafka consumer {}", name);
        List<String> records = new ArrayList<>();
        SafeKafkaConsumer consumer = consumers.get(name);
        if (consumer == null) {
            throw new ResponseStatusException(HttpStatus.NOT_FOUND, "Consumer not found!");
        }
        try {
            consumer.poll(Duration.ofMillis(durationMs)).forEach(record -> records.add(record.value()));
        } catch (WakeupException e) {
            log.trace("Waken up while polling", e);
        }
        return records;
    }

    @DeleteMapping("consumers/{name}")
    public void deleteConsumer(@PathVariable String name) {
        SafeKafkaConsumer consumer = consumers.get(name);
        if (consumer == null) {
            throw new ResponseStatusException(HttpStatus.NOT_FOUND, "Consumer not found!");
        }
        consumer.wakeup();
        consumer.close();
        consumers.remove(name);
    }

    static class SafeKafkaConsumer extends KafkaConsumer<String, String> {

        public SafeKafkaConsumer(Map<String, Object> configs) {
            super(configs);
        }

        @Override
        public synchronized ConsumerRecords<String, String> poll(Duration timeout) {
            return super.poll(timeout);
        }

        @Override
        public synchronized void subscribe(Collection<String> topics) {
            super.subscribe(topics);
        }

        @Override
        public synchronized void close(Duration timeout) {
            super.wakeup();
            super.close(timeout);
        }
    }

    private static class PublishResult {
        public final String topic;
        public final int partition;
        public final long offset;
        public final Instant timestamp;

        private PublishResult(String topic, int partition, long offset, Instant timestamp) {
            this.topic = topic;
            this.partition = partition;
            this.offset = offset;
            this.timestamp = timestamp;
        }
    }
}
